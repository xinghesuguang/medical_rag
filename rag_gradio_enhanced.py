import os
import json
import logging
import gradio as gr
from pathlib import Path
from typing import List, Tuple, Dict
import time

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# from langchain_community.embeddings import DashScopeEmbeddings  # ä¸å†ä½¿ç”¨
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain.schema import Document
from docx import Document as DocxDocument

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedRAGSystem:
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆRAGç³»ç»Ÿ"""
        self.embeddings = None
        self.llm = None
        self.vector_store = None
        self.chain = None
        self.documents = []
        
        # é…ç½®è·¯å¾„
        self.files_dir = Path("/root/autodl-tmp/RAGæ£€ç´¢/file_t")
        self.files_dir.mkdir(exist_ok=True)
        
        # å¾®è°ƒæ¨¡å‹è·¯å¾„
        self.model_path = "/root/autodl-tmp/LLaMA-Factory/output/deepseekr1_7b_lora_sft_dpo"
        
        # åµŒå…¥æ¨¡å‹è·¯å¾„
        self.embedding_model_path = "/root/autodl-tmp/Hugging-Face/models/bge-large-zh"
        
        logger.info("å¢å¼ºç‰ˆRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def load_models(self):
        """åŠ è½½æ‰€éœ€çš„æ¨¡å‹"""
        try:
            logger.info("å¼€å§‹åŠ è½½æ¨¡å‹...")
            
            # åŠ è½½åµŒå…¥æ¨¡å‹ - ä½¿ç”¨DashScope
            # self.embeddings = DashScopeEmbeddings(
            #     model="text-embedding-v2",
            #     dashscope_api_key="sk-8f7884619fd6405f9c90fe16733c597e"
            # )
            
            # åŠ è½½åµŒå…¥æ¨¡å‹ - ä½¿ç”¨æœ¬åœ°bge-large-zh
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_path,
                model_kwargs={'device': 'cuda'},
                encode_kwargs={'normalize_embeddings': True}
            )
            logger.info("åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # åŠ è½½å¾®è°ƒçš„è‡ªå®šä¹‰æ¨¡å‹
            logger.info("å¼€å§‹åŠ è½½å¾®è°ƒæ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                device_map="auto",
                trust_remote_code=True
            ).eval()
            
            llm_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                do_sample=True,
                return_full_text=False,
                pad_token_id=tokenizer.eos_token_id
            )
            self.llm = HuggingFacePipeline(pipeline=llm_pipeline)
            logger.info("å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
            
            return "âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def read_txt_file(self, file_path: Path) -> str:
        """è¯»å–txtæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk') as f:
                return f.read()
    
    def read_docx_file(self, file_path: Path) -> str:
        """è¯»å–docxæ–‡ä»¶"""
        doc = DocxDocument(file_path)
        content = []
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                content.append(paragraph.text.strip())
        return '\n'.join(content)
    
    def read_json_file(self, file_path: Path) -> str:
        """è¯»å–JSONæ–‡ä»¶å¹¶æå–æ–‡æœ¬å†…å®¹ï¼Œæ”¯æŒJSONLæ ¼å¼"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºJSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
            if '\n' in content and not content.startswith('['):
                # JSONLæ ¼å¼å¤„ç†
                texts = []
                for line in content.split('\n'):
                    line = line.strip()
                    if line:
                        try:
                            data = json.loads(line)
                            texts.append(self._extract_text_from_json(data))
                        except json.JSONDecodeError as e:
                            logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ: {line[:50]}... é”™è¯¯: {e}")
                            continue
                return '\n'.join(filter(None, texts))
            else:
                # æ ‡å‡†JSONæ ¼å¼å¤„ç†
                data = json.loads(content)
                return self._extract_text_from_json(data)
                
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read().strip()
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºJSONLæ ¼å¼
            if '\n' in content and not content.startswith('['):
                texts = []
                for line in content.split('\n'):
                    line = line.strip()
                    if line:
                        try:
                            data = json.loads(line)
                            texts.append(self._extract_text_from_json(data))
                        except json.JSONDecodeError as e:
                            logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ: {line[:50]}... é”™è¯¯: {e}")
                            continue
                return '\n'.join(filter(None, texts))
            else:
                data = json.loads(content)
                return self._extract_text_from_json(data)
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æé”™è¯¯: {e}")
            return ""
    
    def _extract_text_from_json(self, data) -> str:
        """ä»JSONæ•°æ®ä¸­é€’å½’æå–æ–‡æœ¬å†…å®¹"""
        texts = []
        
        if isinstance(data, list):
            for item in data:
                texts.append(self._extract_text_from_json(item))
        elif isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, str) and value.strip():
                    texts.append(value.strip())
                elif isinstance(value, (dict, list)):
                    texts.append(self._extract_text_from_json(value))
                elif value is not None:
                    texts.append(str(value))
        elif isinstance(data, str) and data.strip():
            texts.append(data.strip())
        elif data is not None:
            texts.append(str(data))
        
        return '\n'.join(filter(None, texts))
    
    def load_documents(self):
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        try:
            self.documents = []
            supported_extensions = {'.txt', '.docx', '.json'}
            
            files_found = 0
            for file_path in self.files_dir.iterdir():
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    try:
                        if file_path.suffix.lower() == '.txt':
                            content = self.read_txt_file(file_path)
                        elif file_path.suffix.lower() == '.docx':
                            content = self.read_docx_file(file_path)
                        elif file_path.suffix.lower() == '.json':
                            content = self.read_json_file(file_path)
                        
                        if content.strip():
                            # åˆ›å»ºLangChain Documentå¯¹è±¡
                            doc = Document(
                                page_content=content.strip(),
                                metadata={'filename': file_path.name, 'source': str(file_path)}
                            )
                            self.documents.append(doc)
                            files_found += 1
                            logger.info(f"å·²åŠ è½½æ–‡æ¡£: {file_path.name}")
                    
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path.name}: {str(e)}")
            
            if files_found == 0:
                return "âŒ æœªæ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶"
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {files_found} ä¸ªæ–‡æ¡£")
            return f"âœ… æˆåŠŸåŠ è½½ {files_found} ä¸ªæ–‡æ¡£"
            
        except Exception as e:
            error_msg = f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def create_chunks_and_index(self):
        """åˆ›å»ºæ–‡æ¡£å—å¹¶æ„å»ºå‘é‡ç´¢å¼•"""
        try:
            if not self.documents:
                return "âŒ æ²¡æœ‰æ–‡æ¡£ï¼Œè¯·å…ˆåŠ è½½æ–‡æ¡£"
            
            if self.embeddings is None:
                return "âŒ åµŒå…¥æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
            
            logger.info(f"å¼€å§‹å¤„ç† {len(self.documents)} ä¸ªæ–‡æ¡£")
            
            # ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50
            )
            chunks = text_splitter.split_documents(self.documents)
            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æ¡£å—")
            
            # æ£€æŸ¥æ–‡æ¡£å—å†…å®¹
            for i, chunk in enumerate(chunks[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
                logger.info(f"æ–‡æ¡£å— {i+1}: {chunk.page_content[:100]}...")
            
            # æµ‹è¯•åµŒå…¥æ¨¡å‹
            try:
                test_text = "æµ‹è¯•æ–‡æœ¬"
                test_embedding = self.embeddings.embed_query(test_text)
                logger.info(f"åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_embedding)}")
            except Exception as embed_error:
                logger.error(f"åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {embed_error}")
                return f"âŒ åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {embed_error}"
            
            # æ„å»ºFAISSå‘é‡å­˜å‚¨
            logger.info("å¼€å§‹æ„å»ºFAISSå‘é‡å­˜å‚¨...")
            self.vector_store = FAISS.from_documents(
                documents=chunks,
                embedding=self.embeddings
            )
            
            logger.info(f"âœ… åˆ›å»ºäº† {len(chunks)} ä¸ªæ–‡æ¡£å—å¹¶æ„å»ºç´¢å¼•")
            return f"âœ… åˆ›å»ºäº† {len(chunks)} ä¸ªæ–‡æ¡£å—å¹¶æ„å»ºç´¢å¼•"
            
        except Exception as e:
            error_msg = f"âŒ åˆ›å»ºæ–‡æ¡£å—å’Œç´¢å¼•å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return error_msg
    
    def build_chain(self):
        """æ„å»ºé—®ç­”é“¾"""
        try:
            if self.vector_store is None:
                return "âŒ å‘é‡å­˜å‚¨æœªæ„å»ºï¼Œè¯·å…ˆåˆ›å»ºç´¢å¼•"
            
            if self.llm is None:
                return "âŒ LLMæ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
            
            # åˆ›å»ºæ£€ç´¢å™¨ - å¢åŠ æ£€ç´¢æ•°é‡
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}  # æ£€ç´¢å‰5ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
            )
            
            # åˆ›å»ºæç¤ºæ¨¡æ¿ - ç®€åŒ–æ ¼å¼
            template = """æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
            
            prompt_template = ChatPromptTemplate.from_template(template)
            
            # å°†lambdaå‡½æ•°å®šä¹‰ä¸ºå‘½åå˜é‡
            # prompt_to_string = lambda x: x.to_string()
            
            # å®šä¹‰ä¸Šä¸‹æ–‡æ ¼å¼åŒ–å‡½æ•°
            def format_docs(docs):
                return "\n\n".join([doc.page_content for doc in docs])
            
            # æ„å»ºé“¾
            self.chain = (
                RunnableParallel({"context": retriever | format_docs, "question": RunnablePassthrough()})
                | prompt_template
                # | prompt_to_string
                | self.llm
                | StrOutputParser()
            )
            
            logger.info("âœ… é—®ç­”é“¾æ„å»ºå®Œæˆ")
            return "âœ… é—®ç­”é“¾æ„å»ºå®Œæˆ"
            
        except Exception as e:
            error_msg = f"âŒ æ„å»ºé—®ç­”é“¾å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def ask_question(self, question: str) -> str:
        """å›ç­”é—®é¢˜"""
        try:
            if not question or question.strip() == "":
                return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘å°†åŸºäºçŸ¥è¯†åº“ä¸ºæ‚¨æä¾›ç­”æ¡ˆã€‚"
            
            if self.chain is None:
                return "âŒ é—®ç­”é“¾æœªæ„å»ºï¼Œè¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ"
            
            logger.info(f"å¼€å§‹å¤„ç†é—®é¢˜: {question[:50]}...")
            
            # å…ˆæµ‹è¯•æ£€ç´¢åŠŸèƒ½
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            docs = retriever.invoke(question)
            logger.info(f"æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            if docs:
                for i, doc in enumerate(docs):
                    logger.info(f"æ–‡æ¡£ {i+1}: {doc.page_content[:100]}...")
            else:
                logger.warning("æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            
            # è°ƒç”¨é“¾ç”Ÿæˆç­”æ¡ˆ
            answer = self.chain.invoke(question)
            logger.info(f"ç”Ÿæˆçš„ç­”æ¡ˆ: {answer[:100]}...")
            
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²
            if isinstance(answer, str):
                return answer
            else:
                return str(answer)
            
        except Exception as e:
            error_msg = f"âŒ å›ç­”é—®é¢˜æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg)
            return error_msg


# å…¨å±€RAGç³»ç»Ÿå®ä¾‹
rag_system = EnhancedRAGSystem()


def initialize_system():
    """åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ"""
    steps = []
    
    # 1. åŠ è½½æ¨¡å‹
    result = rag_system.load_models()
    steps.append(f"1. åŠ è½½æ¨¡å‹: {result}")
    if "âŒ" in result:
        return "\n".join(steps)
    
    # 2. åŠ è½½æ–‡æ¡£
    result = rag_system.load_documents()
    steps.append(f"2. åŠ è½½æ–‡æ¡£: {result}")
    if "âŒ" in result:
        return "\n".join(steps)
    
    # 3. åˆ›å»ºæ–‡æ¡£å—å’Œç´¢å¼•
    result = rag_system.create_chunks_and_index()
    steps.append(f"3. åˆ›å»ºæ–‡æ¡£å—å’Œç´¢å¼•: {result}")
    if "âŒ" in result:
        return "\n".join(steps)
    
    # 4. æ„å»ºé—®ç­”é“¾
    result = rag_system.build_chain()
    steps.append(f"4. æ„å»ºé—®ç­”é“¾: {result}")
    
    return "\n".join(steps)


def process_query(query, history):
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
    if not query.strip():
        return history, ""
    
    # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
    history.append({"role": "user", "content": query})
    
    # ç”Ÿæˆå›ç­”
    answer = rag_system.ask_question(query)
    
    # æ·»åŠ ç³»ç»Ÿå›ç­”åˆ°å†å²
    history.append({"role": "assistant", "content": answer})
    
    return history, ""


def format_chat_history(history):
    """æ ¼å¼åŒ–èŠå¤©å†å²"""
    if not history:
        return "æš‚æ— å¯¹è¯å†å²"
    
    formatted = []
    for msg in history:
        role = "ğŸ™‹ ç”¨æˆ·" if msg["role"] == "user" else "ğŸ¤– åŠ©æ‰‹"
        formatted.append(f"{role}: {msg['content']}")
    
    return "\n\n".join(formatted)


# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="å¢å¼ºç‰ˆRAGçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸš€ å¢å¼ºç‰ˆRAGçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
        gr.Markdown("""
        è¿™æ˜¯ä¸€ä¸ªåŸºäºLangChainå’Œå¾®è°ƒæ¨¡å‹çš„å¢å¼ºç‰ˆRAGçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿã€‚
        ç³»ç»Ÿæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼ï¼ˆtxtã€docxã€jsonï¼‰ï¼Œä½¿ç”¨DashScopeåµŒå…¥æ¨¡å‹å’Œæ‚¨çš„å¾®è°ƒDeepSeek-R1æ¨¡å‹ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                # ç³»ç»Ÿåˆå§‹åŒ–éƒ¨åˆ†
                gr.Markdown("## ğŸ“‹ ç³»ç»Ÿåˆå§‹åŒ–")
                init_button = gr.Button("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary", size="lg")
                init_status = gr.Textbox(
                    label="åˆå§‹åŒ–çŠ¶æ€",
                    lines=8,
                    interactive=False,
                    placeholder="ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹åˆå§‹åŒ–ç³»ç»Ÿ..."
                )
                
                # èŠå¤©å†å²æ˜¾ç¤º
                gr.Markdown("## ğŸ’¬ å¯¹è¯å†å²")
                chat_history_display = gr.Textbox(
                    label="èŠå¤©è®°å½•",
                    lines=10,
                    interactive=False,
                    placeholder="å¯¹è¯å†å²å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
            
            with gr.Column(scale=3):
                # é—®ç­”äº¤äº’éƒ¨åˆ†
                gr.Markdown("## ğŸ¤” æ™ºèƒ½é—®ç­”")
                
                # èŠå¤©å†å²çŠ¶æ€
                chat_history = gr.State([])
                
                # è¾“å…¥æ¡†å’ŒæŒ‰é’®
                with gr.Row():
                    query_input = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šå®«é¢ˆå£ç²˜è¿æ€ä¹ˆå›äº‹ï¼Ÿ",
                        scale=4
                    )
                    ask_button = gr.Button("ğŸš€ æé—®", variant="primary", scale=1)
                
                # æ¸…ç©ºå¯¹è¯æŒ‰é’®
                clear_button = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")
                
                # ç¤ºä¾‹é—®é¢˜
                gr.Markdown("### ğŸ’¡ ç¤ºä¾‹é—®é¢˜")
                example_questions = [
                    "å®«é¢ˆå£ç²˜è¿æ€ä¹ˆå›äº‹ï¼Ÿ",
                    "å‰åˆ—è…ºç‚åº”è¯¥åƒä»€ä¹ˆé£Ÿç‰©æ¯”è¾ƒå¥½ï¼Ÿ",
                    "ä¸­é£æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ",
                    "éŸ§å¸¦æ’•è£‚è‡ªå·±ä¼šæ„ˆåˆå—ï¼Ÿ",
                    "åµå·¢å›Šè‚¿å¾®åˆ›æ‰‹æœ¯å¤šé•¿æ—¶é—´æ¢å¤ï¼Ÿ"
                ]
                
                for question in example_questions:
                    example_btn = gr.Button(f"ğŸ“ {question}", variant="secondary", size="sm")
                    example_btn.click(
                        fn=lambda q=question: q,
                        outputs=query_input
                    )
        
        # äº‹ä»¶ç»‘å®š
        init_button.click(
            fn=initialize_system,
            outputs=init_status
        )
        
        # æé—®æŒ‰é’®ç‚¹å‡»äº‹ä»¶
        ask_button.click(
            fn=process_query,
            inputs=[query_input, chat_history],
            outputs=[chat_history, query_input]
        ).then(
            fn=format_chat_history,
            inputs=chat_history,
            outputs=chat_history_display
        )
        
        # å›è½¦é”®æé—®
        query_input.submit(
            fn=process_query,
            inputs=[query_input, chat_history],
            outputs=[chat_history, query_input]
        ).then(
            fn=format_chat_history,
            inputs=chat_history,
            outputs=chat_history_display
        )
        
        # æ¸…ç©ºå¯¹è¯
        clear_button.click(
            fn=lambda: ([], ""),
            outputs=[chat_history, chat_history_display]
        )
        
        # æ·»åŠ ä½¿ç”¨è¯´æ˜
        gr.Markdown("""
        ## ğŸ“– ä½¿ç”¨è¯´æ˜
        
        1. **åˆå§‹åŒ–ç³»ç»Ÿ**: é¦–æ¬¡ä½¿ç”¨éœ€è¦ç‚¹å‡»"åˆå§‹åŒ–ç³»ç»Ÿ"æŒ‰é’®ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åŠ è½½æ¨¡å‹å’Œæ–‡æ¡£
        2. **æé—®**: åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥é—®é¢˜ï¼Œç‚¹å‡»"æé—®"æŒ‰é’®æˆ–æŒ‰å›è½¦é”®
        3. **æŸ¥çœ‹å†å²**: å³ä¾§ä¼šæ˜¾ç¤ºå®Œæ•´çš„å¯¹è¯å†å²
        4. **ç¤ºä¾‹é—®é¢˜**: å¯ä»¥ç‚¹å‡»ç¤ºä¾‹é—®é¢˜å¿«é€Ÿä½“éªŒ
        
        ## âš™ï¸ ç³»ç»Ÿç‰¹ç‚¹
        
        - åŸºäºLangChainæ¡†æ¶ï¼Œæ¶æ„æ›´åŠ æ ‡å‡†åŒ–
        - ä½¿ç”¨DashScopeåµŒå…¥æ¨¡å‹å’Œbge-large-zhåµŒå…¥æ¨¡å‹
        - é›†æˆæ‚¨çš„å¾®è°ƒDeepSeek-R1æ¨¡å‹
        - æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼ï¼štxtã€docxã€json
        - å®Œå…¨æœ¬åœ°åŒ–éƒ¨ç½²ï¼Œæ•°æ®å®‰å…¨
        
        ## ğŸ”§ è‡ªå®šä¹‰æ–‡æ¡£
        
        æ‚¨å¯ä»¥å°†è‡ªå·±çš„æ–‡æ¡£æ–‡ä»¶æ”¾å…¥`files`æ–‡ä»¶å¤¹ï¼Œç„¶åé‡æ–°åˆå§‹åŒ–ç³»ç»Ÿã€‚
        
        **æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š**
        - `.txt` æ–‡ä»¶ï¼šçº¯æ–‡æœ¬æ–‡æ¡£
        - `.docx` æ–‡ä»¶ï¼šWordæ–‡æ¡£  
        - `.json` æ–‡ä»¶ï¼šJSONæ ¼å¼æ•°æ®ï¼ˆå¦‚test_encyclopedia.jsonï¼‰
        """)
    
    return interface


if __name__ == "__main__":
    # å¯åŠ¨ç•Œé¢
    interface = create_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7862,  # ä½¿ç”¨ä¸åŒçš„ç«¯å£é¿å…å†²çª
        share=False,
        show_error=True
    )