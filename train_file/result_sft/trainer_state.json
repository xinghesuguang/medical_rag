{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.500089998200036,
  "eval_steps": 500,
  "global_step": 8335,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01499970000599988,
      "grad_norm": 0.660547137260437,
      "learning_rate": 1.1750599520383695e-05,
      "loss": 3.9437,
      "step": 50
    },
    {
      "epoch": 0.02999940001199976,
      "grad_norm": 0.45232605934143066,
      "learning_rate": 2.3741007194244605e-05,
      "loss": 3.5015,
      "step": 100
    },
    {
      "epoch": 0.04499910001799964,
      "grad_norm": 0.4239840805530548,
      "learning_rate": 3.5731414868105514e-05,
      "loss": 3.1782,
      "step": 150
    },
    {
      "epoch": 0.05999880002399952,
      "grad_norm": 0.7254953980445862,
      "learning_rate": 4.772182254196643e-05,
      "loss": 3.1121,
      "step": 200
    },
    {
      "epoch": 0.0749985000299994,
      "grad_norm": 0.9122389554977417,
      "learning_rate": 5.971223021582734e-05,
      "loss": 3.0464,
      "step": 250
    },
    {
      "epoch": 0.08999820003599927,
      "grad_norm": 0.8672909140586853,
      "learning_rate": 7.170263788968825e-05,
      "loss": 2.9809,
      "step": 300
    },
    {
      "epoch": 0.10499790004199916,
      "grad_norm": 1.094333529472351,
      "learning_rate": 8.369304556354916e-05,
      "loss": 2.9478,
      "step": 350
    },
    {
      "epoch": 0.11999760004799905,
      "grad_norm": 1.0074446201324463,
      "learning_rate": 9.568345323741009e-05,
      "loss": 2.9367,
      "step": 400
    },
    {
      "epoch": 0.13499730005399893,
      "grad_norm": 1.033692479133606,
      "learning_rate": 0.00010767386091127098,
      "loss": 2.8684,
      "step": 450
    },
    {
      "epoch": 0.1499970000599988,
      "grad_norm": 1.0797396898269653,
      "learning_rate": 0.00011966426858513192,
      "loss": 2.8666,
      "step": 500
    },
    {
      "epoch": 0.16499670006599867,
      "grad_norm": 0.9114280343055725,
      "learning_rate": 0.00013165467625899283,
      "loss": 2.837,
      "step": 550
    },
    {
      "epoch": 0.17999640007199855,
      "grad_norm": 1.0416374206542969,
      "learning_rate": 0.00014364508393285372,
      "loss": 2.8169,
      "step": 600
    },
    {
      "epoch": 0.19499610007799845,
      "grad_norm": 1.0176221132278442,
      "learning_rate": 0.00015563549160671464,
      "loss": 2.8059,
      "step": 650
    },
    {
      "epoch": 0.20999580008399832,
      "grad_norm": 1.082932710647583,
      "learning_rate": 0.00016762589928057554,
      "loss": 2.8088,
      "step": 700
    },
    {
      "epoch": 0.2249955000899982,
      "grad_norm": 0.8978605270385742,
      "learning_rate": 0.00017961630695443646,
      "loss": 2.7961,
      "step": 750
    },
    {
      "epoch": 0.2399952000959981,
      "grad_norm": 1.0576380491256714,
      "learning_rate": 0.00019160671462829738,
      "loss": 2.7696,
      "step": 800
    },
    {
      "epoch": 0.25499490010199793,
      "grad_norm": 0.9167400598526001,
      "learning_rate": 0.0001999980266118839,
      "loss": 2.7305,
      "step": 850
    },
    {
      "epoch": 0.26999460010799786,
      "grad_norm": 1.023294448852539,
      "learning_rate": 0.00019996294632312767,
      "loss": 2.7658,
      "step": 900
    },
    {
      "epoch": 0.28499430011399773,
      "grad_norm": 0.7335048317909241,
      "learning_rate": 0.0001998840306720897,
      "loss": 2.744,
      "step": 950
    },
    {
      "epoch": 0.2999940001199976,
      "grad_norm": 0.860630452632904,
      "learning_rate": 0.00019976131426455436,
      "loss": 2.7205,
      "step": 1000
    },
    {
      "epoch": 0.3149937001259975,
      "grad_norm": 1.1172726154327393,
      "learning_rate": 0.0001995948509136431,
      "loss": 2.7096,
      "step": 1050
    },
    {
      "epoch": 0.32999340013199735,
      "grad_norm": 0.9025261402130127,
      "learning_rate": 0.00019938471361621694,
      "loss": 2.7023,
      "step": 1100
    },
    {
      "epoch": 0.3449931001379972,
      "grad_norm": 0.8376035690307617,
      "learning_rate": 0.00019913099452086591,
      "loss": 2.7075,
      "step": 1150
    },
    {
      "epoch": 0.3599928001439971,
      "grad_norm": 0.9096546769142151,
      "learning_rate": 0.00019883380488750057,
      "loss": 2.6976,
      "step": 1200
    },
    {
      "epoch": 0.374992500149997,
      "grad_norm": 0.9788839221000671,
      "learning_rate": 0.00019849327503856253,
      "loss": 2.6716,
      "step": 1250
    },
    {
      "epoch": 0.3899922001559969,
      "grad_norm": 0.6466161608695984,
      "learning_rate": 0.00019810955430187631,
      "loss": 2.6946,
      "step": 1300
    },
    {
      "epoch": 0.40499190016199677,
      "grad_norm": 0.7912172675132751,
      "learning_rate": 0.0001976828109451663,
      "loss": 2.7073,
      "step": 1350
    },
    {
      "epoch": 0.41999160016799664,
      "grad_norm": 0.8309360146522522,
      "learning_rate": 0.0001972132321022689,
      "loss": 2.6952,
      "step": 1400
    },
    {
      "epoch": 0.4349913001739965,
      "grad_norm": 0.9857621788978577,
      "learning_rate": 0.00019670102369107127,
      "loss": 2.6599,
      "step": 1450
    },
    {
      "epoch": 0.4499910001799964,
      "grad_norm": 0.8471583127975464,
      "learning_rate": 0.0001961464103232129,
      "loss": 2.6486,
      "step": 1500
    },
    {
      "epoch": 0.46499070018599625,
      "grad_norm": 0.7697312831878662,
      "learning_rate": 0.00019554963520559003,
      "loss": 2.6343,
      "step": 1550
    },
    {
      "epoch": 0.4799904001919962,
      "grad_norm": 1.1622246503829956,
      "learning_rate": 0.00019491096003370522,
      "loss": 2.6293,
      "step": 1600
    },
    {
      "epoch": 0.49499010019799605,
      "grad_norm": 0.7268886566162109,
      "learning_rate": 0.00019423066487690992,
      "loss": 2.6391,
      "step": 1650
    },
    {
      "epoch": 0.5099898002039959,
      "grad_norm": 0.8419437408447266,
      "learning_rate": 0.0001935090480555893,
      "loss": 2.619,
      "step": 1700
    },
    {
      "epoch": 0.5249895002099958,
      "grad_norm": 1.1255006790161133,
      "learning_rate": 0.0001927464260103442,
      "loss": 2.6394,
      "step": 1750
    },
    {
      "epoch": 0.5399892002159957,
      "grad_norm": 0.9841564893722534,
      "learning_rate": 0.00019194313316322668,
      "loss": 2.6227,
      "step": 1800
    },
    {
      "epoch": 0.5549889002219955,
      "grad_norm": 0.8519878387451172,
      "learning_rate": 0.00019109952177109052,
      "loss": 2.6622,
      "step": 1850
    },
    {
      "epoch": 0.5699886002279955,
      "grad_norm": 0.7221731543540955,
      "learning_rate": 0.00019021596177112103,
      "loss": 2.6746,
      "step": 1900
    },
    {
      "epoch": 0.5849883002339953,
      "grad_norm": 0.8422418236732483,
      "learning_rate": 0.00018929284061861167,
      "loss": 2.6287,
      "step": 1950
    },
    {
      "epoch": 0.5999880002399952,
      "grad_norm": 0.7857078313827515,
      "learning_rate": 0.00018833056311705847,
      "loss": 2.6554,
      "step": 2000
    },
    {
      "epoch": 0.614987700245995,
      "grad_norm": 0.6858224868774414,
      "learning_rate": 0.00018732955124064736,
      "loss": 2.6042,
      "step": 2050
    },
    {
      "epoch": 0.629987400251995,
      "grad_norm": 0.6851992011070251,
      "learning_rate": 0.00018629024394921161,
      "loss": 2.5998,
      "step": 2100
    },
    {
      "epoch": 0.6449871002579949,
      "grad_norm": 0.6848260760307312,
      "learning_rate": 0.0001852130969957409,
      "loss": 2.6147,
      "step": 2150
    },
    {
      "epoch": 0.6599868002639947,
      "grad_norm": 0.8316290974617004,
      "learning_rate": 0.0001840985827265262,
      "loss": 2.6635,
      "step": 2200
    },
    {
      "epoch": 0.6749865002699946,
      "grad_norm": 1.102437973022461,
      "learning_rate": 0.00018294718987402848,
      "loss": 2.6047,
      "step": 2250
    },
    {
      "epoch": 0.6899862002759944,
      "grad_norm": 0.8267297744750977,
      "learning_rate": 0.00018175942334256156,
      "loss": 2.6018,
      "step": 2300
    },
    {
      "epoch": 0.7049859002819944,
      "grad_norm": 0.7171293497085571,
      "learning_rate": 0.00018053580398688337,
      "loss": 2.6075,
      "step": 2350
    },
    {
      "epoch": 0.7199856002879942,
      "grad_norm": 0.7258701324462891,
      "learning_rate": 0.00017927686838379285,
      "loss": 2.5532,
      "step": 2400
    },
    {
      "epoch": 0.7349853002939941,
      "grad_norm": 0.6926472783088684,
      "learning_rate": 0.00017798316859683237,
      "loss": 2.6038,
      "step": 2450
    },
    {
      "epoch": 0.749985000299994,
      "grad_norm": 1.1328647136688232,
      "learning_rate": 0.00017665527193419894,
      "loss": 2.576,
      "step": 2500
    },
    {
      "epoch": 0.7649847003059939,
      "grad_norm": 0.7573754787445068,
      "learning_rate": 0.00017529376069997036,
      "loss": 2.6143,
      "step": 2550
    },
    {
      "epoch": 0.7799844003119938,
      "grad_norm": 0.8012339472770691,
      "learning_rate": 0.00017389923193875575,
      "loss": 2.5986,
      "step": 2600
    },
    {
      "epoch": 0.7949841003179936,
      "grad_norm": 0.694350004196167,
      "learning_rate": 0.00017247229717388146,
      "loss": 2.5683,
      "step": 2650
    },
    {
      "epoch": 0.8099838003239935,
      "grad_norm": 0.7114368081092834,
      "learning_rate": 0.0001710135821392287,
      "loss": 2.5442,
      "step": 2700
    },
    {
      "epoch": 0.8249835003299933,
      "grad_norm": 0.6690158247947693,
      "learning_rate": 0.0001695237265048388,
      "loss": 2.5403,
      "step": 2750
    },
    {
      "epoch": 0.8399832003359933,
      "grad_norm": 0.8641137480735779,
      "learning_rate": 0.00016800338359640774,
      "loss": 2.5623,
      "step": 2800
    },
    {
      "epoch": 0.8549829003419932,
      "grad_norm": 0.8657093644142151,
      "learning_rate": 0.0001664532201087924,
      "loss": 2.5281,
      "step": 2850
    },
    {
      "epoch": 0.869982600347993,
      "grad_norm": 0.8230416774749756,
      "learning_rate": 0.00016487391581365384,
      "loss": 2.5423,
      "step": 2900
    },
    {
      "epoch": 0.884982300353993,
      "grad_norm": 0.7282000184059143,
      "learning_rate": 0.00016326616326136677,
      "loss": 2.5594,
      "step": 2950
    },
    {
      "epoch": 0.8999820003599928,
      "grad_norm": 0.7744163870811462,
      "learning_rate": 0.0001616306674773249,
      "loss": 2.5556,
      "step": 3000
    },
    {
      "epoch": 0.9149817003659927,
      "grad_norm": 0.5826953649520874,
      "learning_rate": 0.00015996814565277602,
      "loss": 2.5324,
      "step": 3050
    },
    {
      "epoch": 0.9299814003719925,
      "grad_norm": 0.7763994932174683,
      "learning_rate": 0.00015827932683032202,
      "loss": 2.5526,
      "step": 3100
    },
    {
      "epoch": 0.9449811003779924,
      "grad_norm": 0.7027533054351807,
      "learning_rate": 0.00015656495158422194,
      "loss": 2.5487,
      "step": 3150
    },
    {
      "epoch": 0.9599808003839924,
      "grad_norm": 0.8482690453529358,
      "learning_rate": 0.00015482577169563844,
      "loss": 2.5926,
      "step": 3200
    },
    {
      "epoch": 0.9749805003899922,
      "grad_norm": 0.7972899675369263,
      "learning_rate": 0.0001530625498229693,
      "loss": 2.5523,
      "step": 3250
    },
    {
      "epoch": 0.9899802003959921,
      "grad_norm": 0.8269261121749878,
      "learning_rate": 0.00015127605916740973,
      "loss": 2.5208,
      "step": 3300
    },
    {
      "epoch": 1.00479990400192,
      "grad_norm": 0.8559905886650085,
      "learning_rate": 0.00014946708313389107,
      "loss": 2.5066,
      "step": 3350
    },
    {
      "epoch": 1.0197996040079198,
      "grad_norm": 0.7486523389816284,
      "learning_rate": 0.0001476364149875451,
      "loss": 2.4647,
      "step": 3400
    },
    {
      "epoch": 1.0347993040139196,
      "grad_norm": 0.9783943891525269,
      "learning_rate": 0.0001457848575058445,
      "loss": 2.4697,
      "step": 3450
    },
    {
      "epoch": 1.0497990040199197,
      "grad_norm": 0.8612781763076782,
      "learning_rate": 0.00014391322262657206,
      "loss": 2.436,
      "step": 3500
    },
    {
      "epoch": 1.0647987040259195,
      "grad_norm": 0.6679161190986633,
      "learning_rate": 0.00014202233109177276,
      "loss": 2.3937,
      "step": 3550
    },
    {
      "epoch": 1.0797984040319193,
      "grad_norm": 0.7251501679420471,
      "learning_rate": 0.00014011301208784532,
      "loss": 2.4531,
      "step": 3600
    },
    {
      "epoch": 1.0947981040379193,
      "grad_norm": 0.6185739636421204,
      "learning_rate": 0.0001381861028819305,
      "loss": 2.4577,
      "step": 3650
    },
    {
      "epoch": 1.1097978040439191,
      "grad_norm": 0.6865956783294678,
      "learning_rate": 0.00013624244845475593,
      "loss": 2.4291,
      "step": 3700
    },
    {
      "epoch": 1.124797504049919,
      "grad_norm": 1.1369718313217163,
      "learning_rate": 0.00013428290113009857,
      "loss": 2.4537,
      "step": 3750
    },
    {
      "epoch": 1.139797204055919,
      "grad_norm": 0.9438966512680054,
      "learning_rate": 0.000132308320201027,
      "loss": 2.4157,
      "step": 3800
    },
    {
      "epoch": 1.1547969040619188,
      "grad_norm": 0.7745538353919983,
      "learning_rate": 0.00013031957155308738,
      "loss": 2.4471,
      "step": 3850
    },
    {
      "epoch": 1.1697966040679186,
      "grad_norm": 1.18132746219635,
      "learning_rate": 0.0001283175272845989,
      "loss": 2.4374,
      "step": 3900
    },
    {
      "epoch": 1.1847963040739184,
      "grad_norm": 1.0271965265274048,
      "learning_rate": 0.00012630306532422465,
      "loss": 2.4249,
      "step": 3950
    },
    {
      "epoch": 1.1997960040799185,
      "grad_norm": 0.8136475086212158,
      "learning_rate": 0.00012427706904598543,
      "loss": 2.4708,
      "step": 4000
    },
    {
      "epoch": 1.2147957040859183,
      "grad_norm": 0.7525641918182373,
      "learning_rate": 0.00012224042688188636,
      "loss": 2.4431,
      "step": 4050
    },
    {
      "epoch": 1.2297954040919181,
      "grad_norm": 0.9048141241073608,
      "learning_rate": 0.00012019403193232504,
      "loss": 2.4298,
      "step": 4100
    },
    {
      "epoch": 1.2447951040979182,
      "grad_norm": 1.0456935167312622,
      "learning_rate": 0.00011813878157445252,
      "loss": 2.4033,
      "step": 4150
    },
    {
      "epoch": 1.259794804103918,
      "grad_norm": 0.9173984527587891,
      "learning_rate": 0.00011607557706865911,
      "loss": 2.4635,
      "step": 4200
    },
    {
      "epoch": 1.2747945041099178,
      "grad_norm": 0.8148632645606995,
      "learning_rate": 0.00011400532316335704,
      "loss": 2.3915,
      "step": 4250
    },
    {
      "epoch": 1.2897942041159176,
      "grad_norm": 1.0961838960647583,
      "learning_rate": 0.00011192892769823387,
      "loss": 2.4238,
      "step": 4300
    },
    {
      "epoch": 1.3047939041219174,
      "grad_norm": 0.7826619148254395,
      "learning_rate": 0.00010984730120615004,
      "loss": 2.4071,
      "step": 4350
    },
    {
      "epoch": 1.3197936041279175,
      "grad_norm": 0.8815675973892212,
      "learning_rate": 0.00010776135651385555,
      "loss": 2.3714,
      "step": 4400
    },
    {
      "epoch": 1.3347933041339173,
      "grad_norm": 0.8396595120429993,
      "learning_rate": 0.00010567200834170084,
      "loss": 2.4165,
      "step": 4450
    },
    {
      "epoch": 1.3497930041399173,
      "grad_norm": 0.7176259160041809,
      "learning_rate": 0.00010358017290251698,
      "loss": 2.4346,
      "step": 4500
    },
    {
      "epoch": 1.3647927041459171,
      "grad_norm": 0.9419569969177246,
      "learning_rate": 0.00010148676749984178,
      "loss": 2.4308,
      "step": 4550
    },
    {
      "epoch": 1.379792404151917,
      "grad_norm": 0.6483038663864136,
      "learning_rate": 9.939271012566735e-05,
      "loss": 2.4212,
      "step": 4600
    },
    {
      "epoch": 1.3947921041579168,
      "grad_norm": 0.7584987282752991,
      "learning_rate": 9.729891905788575e-05,
      "loss": 2.407,
      "step": 4650
    },
    {
      "epoch": 1.4097918041639166,
      "grad_norm": 1.0805498361587524,
      "learning_rate": 9.52063124576094e-05,
      "loss": 2.4057,
      "step": 4700
    },
    {
      "epoch": 1.4247915041699166,
      "grad_norm": 0.8898321986198425,
      "learning_rate": 9.311580796654266e-05,
      "loss": 2.4293,
      "step": 4750
    },
    {
      "epoch": 1.4397912041759164,
      "grad_norm": 0.823093593120575,
      "learning_rate": 9.102832230458115e-05,
      "loss": 2.3923,
      "step": 4800
    },
    {
      "epoch": 1.4547909041819165,
      "grad_norm": 0.9237461090087891,
      "learning_rate": 8.894477086781523e-05,
      "loss": 2.4158,
      "step": 4850
    },
    {
      "epoch": 1.4697906041879163,
      "grad_norm": 1.007080316543579,
      "learning_rate": 8.686606732711428e-05,
      "loss": 2.3928,
      "step": 4900
    },
    {
      "epoch": 1.484790304193916,
      "grad_norm": 0.7669705748558044,
      "learning_rate": 8.479312322746694e-05,
      "loss": 2.3684,
      "step": 4950
    },
    {
      "epoch": 1.499790004199916,
      "grad_norm": 1.0975314378738403,
      "learning_rate": 8.272684758825435e-05,
      "loss": 2.3855,
      "step": 5000
    },
    {
      "epoch": 1.5147897042059157,
      "grad_norm": 0.989380955696106,
      "learning_rate": 8.066814650463032e-05,
      "loss": 2.403,
      "step": 5050
    },
    {
      "epoch": 1.5297894042119158,
      "grad_norm": 0.6132025122642517,
      "learning_rate": 7.861792275018388e-05,
      "loss": 2.4032,
      "step": 5100
    },
    {
      "epoch": 1.5447891042179156,
      "grad_norm": 0.9332861304283142,
      "learning_rate": 7.657707538105874e-05,
      "loss": 2.3825,
      "step": 5150
    },
    {
      "epoch": 1.5597888042239156,
      "grad_norm": 0.8383123278617859,
      "learning_rate": 7.454649934170264e-05,
      "loss": 2.4034,
      "step": 5200
    },
    {
      "epoch": 1.5747885042299155,
      "grad_norm": 0.7818053960800171,
      "learning_rate": 7.252708507241971e-05,
      "loss": 2.3645,
      "step": 5250
    },
    {
      "epoch": 1.5897882042359153,
      "grad_norm": 0.8696909546852112,
      "learning_rate": 7.051971811889805e-05,
      "loss": 2.3497,
      "step": 5300
    },
    {
      "epoch": 1.604787904241915,
      "grad_norm": 0.9202345013618469,
      "learning_rate": 6.852527874388381e-05,
      "loss": 2.3826,
      "step": 5350
    },
    {
      "epoch": 1.619787604247915,
      "grad_norm": 0.9235278964042664,
      "learning_rate": 6.654464154117166e-05,
      "loss": 2.3894,
      "step": 5400
    },
    {
      "epoch": 1.634787304253915,
      "grad_norm": 0.9805741906166077,
      "learning_rate": 6.457867505208139e-05,
      "loss": 2.451,
      "step": 5450
    },
    {
      "epoch": 1.6497870042599148,
      "grad_norm": 0.5588842034339905,
      "learning_rate": 6.262824138458857e-05,
      "loss": 2.3929,
      "step": 5500
    },
    {
      "epoch": 1.6647867042659148,
      "grad_norm": 0.9380237460136414,
      "learning_rate": 6.069419583527618e-05,
      "loss": 2.4183,
      "step": 5550
    },
    {
      "epoch": 1.6797864042719146,
      "grad_norm": 0.9379320740699768,
      "learning_rate": 5.8777386514273444e-05,
      "loss": 2.4151,
      "step": 5600
    },
    {
      "epoch": 1.6947861042779144,
      "grad_norm": 0.8506121635437012,
      "learning_rate": 5.687865397334589e-05,
      "loss": 2.3951,
      "step": 5650
    },
    {
      "epoch": 1.7097858042839142,
      "grad_norm": 0.9161887168884277,
      "learning_rate": 5.499883083729958e-05,
      "loss": 2.377,
      "step": 5700
    },
    {
      "epoch": 1.724785504289914,
      "grad_norm": 1.2841829061508179,
      "learning_rate": 5.313874143886214e-05,
      "loss": 2.3515,
      "step": 5750
    },
    {
      "epoch": 1.739785204295914,
      "grad_norm": 0.8438506722450256,
      "learning_rate": 5.129920145719922e-05,
      "loss": 2.3853,
      "step": 5800
    },
    {
      "epoch": 1.754784904301914,
      "grad_norm": 0.9864072799682617,
      "learning_rate": 4.948101756022622e-05,
      "loss": 2.3743,
      "step": 5850
    },
    {
      "epoch": 1.769784604307914,
      "grad_norm": 0.9330993890762329,
      "learning_rate": 4.768498705087134e-05,
      "loss": 2.3979,
      "step": 5900
    },
    {
      "epoch": 1.7847843043139138,
      "grad_norm": 0.9105032682418823,
      "learning_rate": 4.591189751744559e-05,
      "loss": 2.3274,
      "step": 5950
    },
    {
      "epoch": 1.7997840043199136,
      "grad_norm": 1.092979907989502,
      "learning_rate": 4.4162526488272204e-05,
      "loss": 2.3118,
      "step": 6000
    },
    {
      "epoch": 1.8147837043259134,
      "grad_norm": 0.9969317317008972,
      "learning_rate": 4.2437641090728485e-05,
      "loss": 2.3661,
      "step": 6050
    },
    {
      "epoch": 1.8297834043319132,
      "grad_norm": 0.7548059225082397,
      "learning_rate": 4.0737997714847676e-05,
      "loss": 2.4133,
      "step": 6100
    },
    {
      "epoch": 1.8447831043379133,
      "grad_norm": 1.077519178390503,
      "learning_rate": 3.9064341681629913e-05,
      "loss": 2.3826,
      "step": 6150
    },
    {
      "epoch": 1.859782804343913,
      "grad_norm": 0.9788841009140015,
      "learning_rate": 3.741740691620692e-05,
      "loss": 2.3869,
      "step": 6200
    },
    {
      "epoch": 1.8747825043499131,
      "grad_norm": 0.9637579917907715,
      "learning_rate": 3.579791562600415e-05,
      "loss": 2.4107,
      "step": 6250
    },
    {
      "epoch": 1.889782204355913,
      "grad_norm": 0.8382920622825623,
      "learning_rate": 3.420657798404097e-05,
      "loss": 2.4036,
      "step": 6300
    },
    {
      "epoch": 1.9047819043619127,
      "grad_norm": 0.7615219950675964,
      "learning_rate": 3.264409181750847e-05,
      "loss": 2.3822,
      "step": 6350
    },
    {
      "epoch": 1.9197816043679126,
      "grad_norm": 0.8176096677780151,
      "learning_rate": 3.111114230176102e-05,
      "loss": 2.354,
      "step": 6400
    },
    {
      "epoch": 1.9347813043739124,
      "grad_norm": 0.9089095592498779,
      "learning_rate": 2.9608401659855778e-05,
      "loss": 2.3605,
      "step": 6450
    },
    {
      "epoch": 1.9497810043799124,
      "grad_norm": 1.0537388324737549,
      "learning_rate": 2.813652886777217e-05,
      "loss": 2.448,
      "step": 6500
    },
    {
      "epoch": 1.9647807043859122,
      "grad_norm": 0.8767223358154297,
      "learning_rate": 2.6696169365440116e-05,
      "loss": 2.3736,
      "step": 6550
    },
    {
      "epoch": 1.9797804043919123,
      "grad_norm": 1.1388176679611206,
      "learning_rate": 2.528795477370438e-05,
      "loss": 2.3403,
      "step": 6600
    },
    {
      "epoch": 1.994780104397912,
      "grad_norm": 0.9650731682777405,
      "learning_rate": 2.3912502617348553e-05,
      "loss": 2.3672,
      "step": 6650
    },
    {
      "epoch": 2.00959980800384,
      "grad_norm": 0.7481647729873657,
      "learning_rate": 2.257041605430067e-05,
      "loss": 2.2968,
      "step": 6700
    },
    {
      "epoch": 2.02459950800984,
      "grad_norm": 0.9060846567153931,
      "learning_rate": 2.126228361113839e-05,
      "loss": 2.2263,
      "step": 6750
    },
    {
      "epoch": 2.0395992080158396,
      "grad_norm": 0.7401612997055054,
      "learning_rate": 1.998867892501105e-05,
      "loss": 2.2609,
      "step": 6800
    },
    {
      "epoch": 2.0545989080218394,
      "grad_norm": 0.8843700885772705,
      "learning_rate": 1.8750160492090173e-05,
      "loss": 2.2628,
      "step": 6850
    },
    {
      "epoch": 2.0695986080278392,
      "grad_norm": 1.0425314903259277,
      "learning_rate": 1.7547271422660094e-05,
      "loss": 2.283,
      "step": 6900
    },
    {
      "epoch": 2.0845983080338395,
      "grad_norm": 0.9243085384368896,
      "learning_rate": 1.6380539202955257e-05,
      "loss": 2.2431,
      "step": 6950
    },
    {
      "epoch": 2.0995980080398393,
      "grad_norm": 0.8752656579017639,
      "learning_rate": 1.5250475463849146e-05,
      "loss": 2.281,
      "step": 7000
    },
    {
      "epoch": 2.114597708045839,
      "grad_norm": 0.9935040473937988,
      "learning_rate": 1.41575757564957e-05,
      "loss": 2.2638,
      "step": 7050
    },
    {
      "epoch": 2.129597408051839,
      "grad_norm": 0.7839491963386536,
      "learning_rate": 1.3102319335022251e-05,
      "loss": 2.3022,
      "step": 7100
    },
    {
      "epoch": 2.1445971080578388,
      "grad_norm": 0.8110079169273376,
      "learning_rate": 1.2085168946368997e-05,
      "loss": 2.3052,
      "step": 7150
    },
    {
      "epoch": 2.1595968080638386,
      "grad_norm": 1.1244460344314575,
      "learning_rate": 1.1106570627366708e-05,
      "loss": 2.2574,
      "step": 7200
    },
    {
      "epoch": 2.1745965080698384,
      "grad_norm": 0.8439898490905762,
      "learning_rate": 1.0166953509142718e-05,
      "loss": 2.2961,
      "step": 7250
    },
    {
      "epoch": 2.1895962080758387,
      "grad_norm": 0.8641679286956787,
      "learning_rate": 9.266729628939785e-06,
      "loss": 2.2819,
      "step": 7300
    },
    {
      "epoch": 2.2045959080818385,
      "grad_norm": 1.0889042615890503,
      "learning_rate": 8.406293749431249e-06,
      "loss": 2.2431,
      "step": 7350
    },
    {
      "epoch": 2.2195956080878383,
      "grad_norm": 0.8910419344902039,
      "learning_rate": 7.5860231856111356e-06,
      "loss": 2.3014,
      "step": 7400
    },
    {
      "epoch": 2.234595308093838,
      "grad_norm": 1.138466238975525,
      "learning_rate": 6.806277639335612e-06,
      "loss": 2.2594,
      "step": 7450
    },
    {
      "epoch": 2.249595008099838,
      "grad_norm": 1.089972972869873,
      "learning_rate": 6.067399041587685e-06,
      "loss": 2.3014,
      "step": 7500
    },
    {
      "epoch": 2.2645947081058377,
      "grad_norm": 1.0128766298294067,
      "learning_rate": 5.369711402535238e-06,
      "loss": 2.2845,
      "step": 7550
    },
    {
      "epoch": 2.279594408111838,
      "grad_norm": 1.243646502494812,
      "learning_rate": 4.713520669447147e-06,
      "loss": 2.2512,
      "step": 7600
    },
    {
      "epoch": 2.294594108117838,
      "grad_norm": 0.8206391334533691,
      "learning_rate": 4.099114592530518e-06,
      "loss": 2.2584,
      "step": 7650
    },
    {
      "epoch": 2.3095938081238376,
      "grad_norm": 0.9135885238647461,
      "learning_rate": 3.5267625987477017e-06,
      "loss": 2.3144,
      "step": 7700
    },
    {
      "epoch": 2.3245935081298374,
      "grad_norm": 0.9063549637794495,
      "learning_rate": 2.996715673668027e-06,
      "loss": 2.2576,
      "step": 7750
    },
    {
      "epoch": 2.3395932081358373,
      "grad_norm": 1.092633605003357,
      "learning_rate": 2.5092062514067926e-06,
      "loss": 2.2735,
      "step": 7800
    },
    {
      "epoch": 2.354592908141837,
      "grad_norm": 1.1855971813201904,
      "learning_rate": 2.064448112699002e-06,
      "loss": 2.2627,
      "step": 7850
    },
    {
      "epoch": 2.369592608147837,
      "grad_norm": 1.003345012664795,
      "learning_rate": 1.6626362911531635e-06,
      "loss": 2.3167,
      "step": 7900
    },
    {
      "epoch": 2.384592308153837,
      "grad_norm": 1.0261733531951904,
      "learning_rate": 1.303946987725868e-06,
      "loss": 2.2818,
      "step": 7950
    },
    {
      "epoch": 2.399592008159837,
      "grad_norm": 1.0561543703079224,
      "learning_rate": 9.885374934548663e-07,
      "loss": 2.2717,
      "step": 8000
    },
    {
      "epoch": 2.414591708165837,
      "grad_norm": 0.9143152236938477,
      "learning_rate": 7.165461204843737e-07,
      "loss": 2.2908,
      "step": 8050
    },
    {
      "epoch": 2.4295914081718366,
      "grad_norm": 0.8231282234191895,
      "learning_rate": 4.88092141412988e-07,
      "loss": 2.3188,
      "step": 8100
    },
    {
      "epoch": 2.4445911081778364,
      "grad_norm": 0.9885844588279724,
      "learning_rate": 3.032757369907269e-07,
      "loss": 2.2844,
      "step": 8150
    },
    {
      "epoch": 2.4595908081838362,
      "grad_norm": 0.9686741232872009,
      "learning_rate": 1.6217795218818054e-07,
      "loss": 2.2606,
      "step": 8200
    },
    {
      "epoch": 2.474590508189836,
      "grad_norm": 1.1292898654937744,
      "learning_rate": 6.486066065694018e-08,
      "loss": 2.3078,
      "step": 8250
    },
    {
      "epoch": 2.4895902081958363,
      "grad_norm": 1.1043487787246704,
      "learning_rate": 1.1366537597046201e-08,
      "loss": 2.2825,
      "step": 8300
    },
    {
      "epoch": 2.500089998200036,
      "step": 8335,
      "total_flos": 3.17283581923269e+18,
      "train_loss": 2.503671519114146,
      "train_runtime": 17116.7548,
      "train_samples_per_second": 14.606,
      "train_steps_per_second": 0.487
    }
  ],
  "logging_steps": 50,
  "max_steps": 8335,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.17283581923269e+18,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
